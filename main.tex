\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
% PACKAGE FIGURE
\usepackage{caption}
\usepackage{graphicx}
% PACKAGE MARGE
\usepackage[top=2cm, bottom=2cm, left=2.5cm, right=2.5cm]{geometry}
% PACKAGE COULEUR
%\usepackage{color}
\usepackage{xcolor}
\usepackage{amsmath}
% PACKAGE POUR LE WARNING
\newcommand\Warning{%
 \makebox[1.4em][c]{%
 \makebox[0pt][c]{\raisebox{.1em}{\small!}}%
 \makebox[0pt][c]{\color{red}\Large$\bigtriangleup$}}}%
% PACKAGE ENCADRE
\usepackage{calc}
\usepackage{tcolorbox}
\newtcolorbox{mybox1}[1]{colback=red!5!white,colframe=red!75!black,fonttitle=\bfseries,title=#1}
\newtcolorbox{mybox2}[1]{colback=blue!5!white,colframe=blue!75!black,fonttitle=\bfseries,title=#1}
\newtcolorbox{mybox3}[1]{colback=green!5!white,colframe=green!75!black,fonttitle=\bfseries,title=#1}
\newtcbox{\mybox}{nobeforeafter,colframe=black,colback=black!10!white,boxrule=0.5pt,arc=4pt,
  boxsep=0pt,left=6pt,right=6pt,top=6pt,bottom=6pt,tcbox raise base}
\newtcolorbox{mybox4}[1]{colback=white!5!white,colframe=white!75!black,fonttitle=\bfseries,title=#1}

\usepackage[sort&compress]{natbib}
\citestyle{nature}

% PACKAGE COLONNE
\usepackage{multirow}
% PACKAGE NUMEROTATION PAGE
\usepackage{fancyhdr}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

% DEBUT DU DOCUMENT
% PAGE DE PRESENTATION
\begin{document}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.2]{EPFL-logo.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.75]{lcmd-logo.jpg}
\end{minipage}
\vspace{0.5cm}
\hrule 
\vspace{0.5cm}
\begin{center}
\LARGE \textbf{\textcolor{red}{M}olecular \textcolor{red}{D}ynamics meets \textcolor{red}{N}eural \textcolor{red}{N}etworks}: \\
\LARGE \textbf{TUTORIAL}
\vspace{0.5cm}
\\ % The assignment title
\end{center}
\hrule 
\vspace{2cm}
\begin{center}
\includegraphics[scale=0.17]{Picture.jpeg}
\end{center}
\vspace{1cm}
\begin{center}
    Frédéric CELERSE \\
    Veronika JURÁSKOVÁ \\
    Rubén LAPLAZA \\
    Clémence CORMINBOEUF
\end{center}
%\vspace{2cm}
%\begin{mybox1}{}
%\Warning Please visit the website: \url{https://toto-is-coming-soon} to obtain last updates of this tutorial and information about other new tutorials.
%\end{mybox1}
 
% FIN PAGE DE PRESENTATION

% TABLE DES MATIERES
\newpage
\tableofcontents

% DEBUT DU TUTORIEL
\newpage
% SECTION INTRODUCTION
\pagestyle{fancy}
\renewcommand\headrulewidth{1pt}
\fancyhead[R]{MD combining NN: tutorial}
\renewcommand\footrulewidth{1pt}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\today}
\section{Introduction}
\fancyhead[L]{1. Introduction}
This tutorial aims to guide the building of neural network-based potentials (NNPs) and their application in molecular dynamics (MD). As the traditional \textit{ab initio} MD is too computationally demanding to obtain statistically converged simulations of chemical reactions and processes in large systems, the MD with NNPs benefits from the much lower cost of the simulation while the accuracy of the AIMD is conserved.  
In this regard, NNPs have been already used in the reactive simulations in the gas phase and aqueous solutions (\textit{e.g.}, in the description of proton transfer and urea hydrolysis). 

\textbf{Part 1} of this tutorial is dedicated to the generation of the Behler-Parrinello NNPs, including the preparation of a robust data set, selection of the descriptors and monitoring of the accuracy of the training. \textbf{Part 2} demonstrates how to use the NNPs using the i--Pi interface to LAMMPS and other codes. Finally, \textbf{Part 3} introduces the concept of baselined NNPs and showcases its advantages compared to NNPs trained directly on DFT data. 


\fancyhead[L]{2. Neural Networks-based potentials: Crash course}

\section{Neural Networks-based potentials: Crash course}

\textbf{Neural Networks are statistical models which can reproduce complex non-linear functions using large amounts of so-called training points.} The availability of so-called \textit{Big data} in a broad range of human activities resulted in the harvesting of neural networks in the development of game-playing models, speech and text recognition software, or analysis of patients' data in modern health care, to name a few. Recently, the power of NN to reproduce highly complex data has been exploited in the development of a new generation of potentials for molecular dynamics. The Neural Network Potentials (NNPs) are trained to mimic the shape of the corresponding potential energy surface using the \textbf{data sets} containing coordinates and corresponding \textit{ab initio} data (i.e., energy and possibly forces). The resulting NNPs are then used to propagate molecular dynamics with the accuracy of the underlying \textit{ab initio} method but with a fraction of its original cost. Many NN architectures to train the NNPs have been proposed during the last years. In this tutorial, we focus on the training of Behler-Parrinello NNPs based on feedforward NN.  

\subsection{Feedforward Neural Networks}
\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.4]{NNP_scheme.jpeg}
    \caption{Schematic representation of a small feedforward NN. Nodes are interconnected in order to establish a functional relation between a set of coordinates $G$ (input layer) and the potential energy of the structure E (output layer). }
    \label{NNP_scheme}
\end{figure}
The example of the feedforward NN is depicted in Fig. \ref{NNP_scheme}. It is made from several nodes organized in interconnected layers. The potential energy \textit{E} corresponds to the node in the \textbf{output layer} and the atomic coordinates of a frame are provided in the \textbf{input layer} as vectors of input coordinates $G=\{G_i\}$. The input and output layers are connected via several so-called \textbf{hidden layers} with no physical meaning. The number of layers and the nodes define the flexibility of the final NN. The example shows the NN with two layers containing five and four nodes, respectively. 
Each node is connected to the nodes of the next layer with a specific weight \textbf{$a_{ij}^{kl}$}, where $i$ is the index of a node of layer $k$, which is connected to the node with index $j$ of layer $l$. For instance, the $a_{45}^{01}$ connects the nodes $G_4$ ($k = 0$, $i=4$) and $y_5^1$ ($l=1$,  $j=5$). Nodes in hidden and output layers are connected to a so-called bias node, which scales their values by a bias weight b$_i^j$, where $j$ is the layer of the target node and $i$ is the node index. In our scheme, the bias node is omitted for simplicity.

The value y$_i^j$ of any node is computed as:
\begin{equation}
    y_i^{j} = f_i^j(b_i^j + \sum_{k=1}^{N_{j-1}} a_{k,i}^{j-1,j} y_k^{j-1})
\end{equation}
The equation is a linear combination of the values of all nodes in the previous layer scaled by the bias weight. The term f$_i^j$ represents the \textbf{activation function} of the NN. The activation function ensures the ability to fit complex non-linear functions. The examples of activation functions are:
\begin{itemize}
    \item hyperbolic tangent,
    \item sigmoid function,
    \item Gaussian function,
    \item Softplus
    \item trigonometric functions
    \item exponential function.
\end{itemize}
According to Figure~\ref{NNP_scheme}, the complete functional form for the energy is given by:
\begin{equation}
    E = f_1^3(b_1^3 + \sum_{k=1}^4 a_{k1}^{23} f_k^2 (b_k^2 + \sum_{j=1}^5 a{jk}^{12} f_j^1 (b_j^1 + \sum_{i=1}^4 a_{ij}^{01} G_i)))
\end{equation}

\subsection{High--dimensional NNP}
\begin{figure}[!htp]
    \centering
    \includegraphics[scale=0.7]{High-dimensional-NNP_scheme.jpeg}
    \caption{High dimensional NN scheme: Atomic positions are represented by a vector of symmetry functions $G_i$. }
    \label{High-dimensional-NNP_scheme}
\end{figure}
Feedforward neural networks discussed in the previous section suffer from several drawbacks preventing their application in the simulation of complex chemical systems. The limitations are, for instance:
\begin{enumerate}
    \item absence of symmetry between equivalent terms (for instance, the OH bonds of a water molecule)
    \item system size dependency: no atoms could be deleted or added as it causes problems in the connecting weights.
\end{enumerate}
In 2007, Behler and Parrinello proposed a solution to this problem by expressing the total energy $E_s$ as a sum of the atomic energy contributions E$_i$.\cite{Behler2007}
\begin{equation}
    E_s = \sum_{i=1}^{N_{atom}} E_i
\end{equation}
Starting from the Cartesian coordinates $R$, every atom is represented by a set of atom-centered symmetry functions $G_i$, which describe the atoms' chemical environment within a given cutoff. Each atom is then assigned a feedforward NN yielding the atomic contribution to the total energy. The weights of the NN are identical for atoms of the same element to ensure that the trained NNPs do not depend on the number and order of the atoms.
%
\subsection{Atom-centered symmetry functions}

Together with the NN topology, the seminal work of Behler and Parrinello introduced also new types of symmetry functions to characterize the local environment of each atom. The so-called Atom-centered symmetry functions (ACSFs)\cite{behl11jcp} are constructed from the positions of the atoms and their closest neighbors. This representation guarantees that the atoms with the same environment yield the same atomic energy contribution which is also translationally and rotationally invariant. 

The typically used ACSFs are two-body radial and three-body angular functions in a form:

\begin{equation}
    G_i^2  = \sum_i e^{-\eta (R_{ij}-R_s)^2} \cdot f_c(R_{ij}) \label{eq:radial}
\end{equation}
\begin{equation}
    G_i^3  = 2^{(1-\xi)} \sum_{j,k \neq i}^{\mathrm{all}}(1 + \lambda \mathrm{cos}\;\theta_{ijk})^{\zeta} \cdot e^{-\eta (R_{ij}^2 + R_{ik}^2 + R_{jk}^2)} \cdot f_c(R_{ij}) \cdot f_c(R_{ik}) \cdot f_c(R_{jk}),
    \label{eq:angular}
\end{equation}

where $f_c$ is a cutoff function ensuring the smooth decay of the symmetry function to zero.   

In this tutorial, we use the original Behler-Parrinello ACSFs. However, during the last years, many modifications to the original ACSFs were proposed as well as new types of functional forms to describe the atomic environments. Various types of structural representations are summarized for example in Ref. \citenum{Musil2021}.

\subsection{Constructing and training NN}
The general protocol to construct NNP is as follows:
\begin{enumerate}
    \item \textbf{Define an initial set of structures and compute the reference energies and forces.} The electronic structure method used as a reference should be accurate enough to describe the studied problem.
    \item T\textbf{rain the first version of NNPs.} The fraction of the data set (\textit{e.g.} 80 \%) is used in the NNP training (so-called training set) while the remaining part serves as a test set to validate the error on unseen data. Ideally, several different NNP with different training sets should be trained to identify potential problems in the training set.
    \item \textbf{Perform preliminary simulations using the NNPs} to evaluate the stability of the NNP and identify underrepresented areas of the PES, \textit{e.g.} structures triggering extrapolation warnings or unphysical geometries.
    \item \textbf{Isolate the problematic structures}, compute reference data and add them to the training set and re-train the NNP.
    \item Repeat the validation and re-training of the NNP until no instabilities are present.
\end{enumerate}

For a detailed general tutorial review on Behler-Parrinello NN, see Ref. \citenum{Behler2015tutorial}. 

\newpage
%

\fancyhead[L]{3. Requirements}
\section{Requirements}
This tutorial presents a workflow which requires extensive number of different software packages, scripts and libraries. The versions of the codes used here are listed below.

\subsection{Software}
All the codes are available at GitHub (see the last section of tutorial):
\begin{itemize}
    \item n2p2 (version 1.0.0)
    \item LAMMPS (version 2)
    \item i--PI (version 2.0)
    \item Tinker (version 8.8)
    \item cp2k (version 6.1)
\end{itemize}
%
\subsection{Dependencies}
\begin{itemize}
    \item python (version 2.7 and 3.6)
    \item libraries intel, intel-mpi, intel-mkl, gsl, eigen, gc,c mvapich2, openblas, n2p2/lib
\end{itemize}

\newpage
%
\fancyhead[L]{4. Building NNPs}
\section{Part 1: How to generate neural network potentials}
\subsection*{Instructions}

Currently, there is no universal NNP that is applicable to model all classes of systems. Despite the ongoing efforts in the training of general neural network-based force fields, many systems require building the NNPs from scratch. In the following section, we provide general instructions on how to train NNP using the Behler-Parrinello approach. The required steps include: 

\begin{enumerate}
    \item Sampling of the phase space to produce initial training structure;
    \item Generating a large pool of symmetry functions;
    \item Identification of a representative set of fingerprints;
    \item Careful selection of a small set of structures for the training of NN;
    \item Computation of reference forces and energies for the selected structures;
    \item Training of the NN.
\end{enumerate}

The training protocol is summarized in the Fig. \ref{protocol}.
\begin{figure} [!htp]
    \centering
    \includegraphics[scale=1.5]{protocol.png}
    \caption{Scheme of the NNP preparation.}
    \label{protocol}
\end{figure}

\newpage
\subsection{Step 0: The system}
\begin{figure} [!htp]
    \centering
    \includegraphics[scale=0.05]{watertiny.jpeg}
    \caption{System containing 27 molecules in a cubic box with edge of 9.32 \AA.}
    \label{Rep_sys}
\end{figure}

As a relatively simple test case, we train the NNPs for a small water box containing 27 water molecules with a cell size of 9.32 \AA. These systems are composed of only two elements, \textit{i.e.} H and O, which significantly simplifies the training.

\subsection{Step 1: Generating the data}
\begin{mybox2}{{Files needed}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.2]{tinker.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item system.xyz
    \item system.key
    \item system.dyn
    \item water03.prm
    \item \textit{dynamic} module
\end{itemize}
\end{minipage}
\end{mybox2}

The first step in the tutorial is the preparation of the training set. The training set can contain systems of different sizes and different compositions. To keep things simple, we will train the NNPs using only one size of the water box. To generate an extensive set of the structures, it is beneficial to use molecular dynamics starting either from equilibrated structure or from experimental data (\textit{e.g.}, X-ray). Random displacement of the atoms could be also applied.

As the first training set should be large enough and contain representative geometries, the potential for the exploratory dynamics should provide a reasonable cost/accuracy ratio. For example, classical force fields might not be the best choice, as they can sample structures that are too different from the ones obtained at the reference level. The suitable compromise is, therefore, the application of semiempirical methods or polarizable force fields.

Herein we generate the system using AMOEBA polarizable force-field as available in Tinker. The files required for the simulation are *.xyz,  *.key, and *.prm. For more information on the preparation of the files for Tinker, see tutorials listed in section \ref{sec:tutorials}.

The files contain:

\begin{itemize}
    \item The *.xyz file - the Cartesian coordinates of the initial water box in the Tinker xyz format (Caution! Tinker xyz differs significantly from standard xyz or extended xyz formats.)  
    \item The *.key file - the settings to run the dynamics.
    \item The *.dyn - positions, velocities, and accelerations of the last frame of the dynamics.
    \item The *.prm file - AMOEBA parameters (all the terms to compute the potential energy of a specific structure)
\end{itemize}

MD using Tinker can be launched as:
\begin{center}
\mybox{\textit{dynamic system 1000000 2.0 1 2 300 $>$ system.out}}
\end{center}

This command means that the simulation will run using the \textit{dynamic} module of Tinker for 1000000 steps with a time step of 2.0 fs (\textit{i.e.}, the total length of the simulation is 2 ns). The next term specifies the frequency (in ps) of saving the trajectory to the *.arc file (here we save it every 1 ps). The final \textit{system.arc} file will thus contain 2000 frames. The following term specifies the mode of the dynamics, 2 corresponds to NVT canonical ensemble. The last term corresponds to the simulation temperature (300 K). 

\begin{mybox1}{Enhanced sampling}
\Warning As the system is quite simple, the standard molecular dynamics are sufficient to provide an extensive sampling. In more complex cases, however, the simulations starting from different initial structures or using enhanced sampling techniques might be needed to ensure that all the relevant structures are present in the training set. Suitable simulations techniques include, for instance:
\begin{enumerate}
    \item Replica exchange molecular dynamics
    \item Sampling techniques using bias, such as metadynamics, steered molecular dynamics, umbrella sampling, ...
    \item Transition path sampling
\end{enumerate}
\end{mybox1}
\begin{mybox1}{Extending the database with more distorted structures}

\Warning The NNPs have in general very limiting extrapolation capacity. As they are trained only on the information included in the training set, it is beneficial to consider more distorted structures covering the repulsive and dissociative parts of the PES. Distorted structures can be generated by simulations using, for example: 
\begin{enumerate}
    \item High temperature
    \item High pressure
    \item Path integral molecular dynamics
    \item Specific constraints and restraints
    \item Random displacement
\end{enumerate}

\end{mybox1}
\begin{mybox3}{Output Files}
\begin{itemize}
    \item system.out
    \item \textbf{system.arc} (file containing the structures)
    \item system.dyn
\end{itemize}
\end{mybox3}
%
\subsection{Step 2: Descriptor selection}
To assign the structures with a suitable set of descriptors, we generate a large pool of the ACSFs and select a small representative set using, for example, CUR decomposition.

\subsubsection{Generating symmetry functions}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{Python-fortran.jpeg}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item system.arc
    \item \textcolor{red}{from\_txyz\_to\_n2p2.f90}
    \item \textcolor{red}{symmetry\_functions.py}
\end{itemize}
\end{minipage}
\end{mybox2}

As a first step, place the structures generated in the previous part to the \textit{system.arc} file. 

Structures in this file can be converted to input files for n2p2 as follows:

\begin{enumerate}
    \item compile the fortran code \textcolor{red}{from\_txyz\_to\_n2p2.f90}:
    \textit{gfortran from\_txyz\_to\_gen.f90 -o from\_txyz\_to\_gen} 
    \item place the system.arc file in the same directory of the \textcolor{green}{from\_txyz\_to\_gen} executable and launch it as \textit{./from\_txyz\_to\_gen}
\end{enumerate}
The script generates input.data file for the n2p2. The definition of each frame starts with a short header:
\vspace{0.5cm}

 \noindent begin \\
 comment \\
 lattice 17.612 0.0 0.0 \\
 lattice 0.0 17.612 0.0 \\
 lattice 0.0 0.0 17.612 \\

\noindent "lattice" defines the dimension of the box in Bohr.
Each atom of the structure is defined as: \\ \\
 atom \hspace{0.5cm}  -4.367  \hspace{0.5cm}      5.267    \hspace{0.5cm}    4.603  \hspace{0.5cm}    O \hspace{0.5cm} 0 0 \hspace{0.5cm} 0.0 0.0 0.0 \\ \\
The keyword "atom" specifies that the line corresponds to one atom, -4.367, 5.267, 4.603 are the xyz coordinates of the atom in Bohr, O is the element label, the following two columns 0 0 are not used. Finally, the last three columns 0.0 0.0 0.0 are the xyz force components acting on the atom. In this example, forces are set to 0.0 since they are not used.
\vspace{0.5cm}

 \noindent The frame ends with: 
\vspace{0.5cm}

 \noindent energy 0.000 \\
 charge 0.0 \\
 end \\

\noindent where "energy" is the reference total energy of the system, "charge" is the total charge of the system, and "end" closes the definition of the structure. 

\begin{mybox1}{Units in n2p2}
\Warning As indicated on the n2p2 website, the units correspond to the physical units defined in the input files. As we combine several codes with possibly different unit specifications, keep track of the units and stay consistent within the workflow.

In the inputs for n2p2, we use:
\begin{enumerate}
    \item lattice: \textbf{Bohr}
    \item x/y/z positions: \textbf{Bohr}
    \item x/y/z forces: \textbf{Hartree Bohr$^{-1}$}
    \item energy: \textbf{Hartree}
\end{enumerate}

\textcolor{red}{The conversion from Angström to Bohr is done by a multiplication by a factor \textbf{1.88973}}
\end{mybox1}

All settings and parameters for the training of NNP are specified in \textbf{input.nn} file. It contains three main parts:
\begin{enumerate}
    \item GENERAL NNP SETTINGS
    \item ADDITIONAL SETTINGS FOR TRAINING
    \item SYMMETRY FUNCTIONS
\end{enumerate}

GENERAL NNP SETTINGS specify the general set-up of the neural network, for instance, a number of the elements, layers and nodes, cut-offs, scaling normalization, etc.

ADDITIONAL SETTINGS FOR TRAINING define explicitly the parameters of the training as the number of epochs, fraction of energies used in the training, and type of the optimizing algorithm. 

The last part defines the symmetry functions sets used in training. While the parameters for the training can be found in the literature and possible modification directly depend on the simulated system, the symmetry functions are often built completely from scratch and require careful selection.

The n2p2 currently supports Behler-Parrinello types of symmetry functions (radial, angular, wide angular, for definition see Ref. \citenum{behl11jcp}), their weighted variants (Ref. \citenum{Gastegger2018}) and polynomial symmetry functions (Ref. \citenum{Bircher2021}). In this tutorial, we use standard Behler-Parrinello functions.

The initial set of the symmetry function can be generated, for example, by a \textit{symfunc\_paramgen.py} code by Florian Buchner available at \url{https://github.com/flobuch/n2p2/tree/symfunc_paramgen}. Jupyter notebook with an example of the use of a the code is provided at \url{https://github.com/flobuch/n2p2/blob/symfunc_paramgen/tools/python/symfunc_paramgen/examples/example.ipynb}.

For the NNP of water box, we generate set of radial symmetry functions with cutoff $r_c = 4, 8, 12$ Bohr and $N = 8$ and two sets of angular symmetry functions following the protocol describe in Ref. \citenum{Imbalzano2018}.  Store the symmetry functions in one file \textit{e.g.},  asfs.txt and append it at the end of the input.nn.

\begin{center}
\mybox{\textit{cat asfs.txt $>>$ input.nn}}
\end{center}

%Initial set of the symmetry function can be generate by symmetry\_functions.py as

%\begin{center}
%\mybox{\textit{python symmetry\_functions.py $>$ asfs.txt}}
%\end{center}
%Without any arguments, \textcolor{red}{symfun\_gen.py} generates the data by default: ASFs for only H atom, with a cutoff of 12.0 Bohr and a delta of 0.05. If you want more (as it surely the case), please add in the sumbission line the following different options:
%\begin{itemize}
%    \item -e: the name of elements we want the symmetry functions (default: H) (example in this case: -e H O)
%    \item -c: the cutoff for the symmetry functions (default: 12.0) (example in this case: -c 14.0)
%    \item -d: the ratio between the minimum sigma of the gaussian and the cutoff. It determines the accuracy with which you sample the space. (default: 0.05) (example in this case: -d 0.2)
%\end{itemize}

%According to these parameters and the system we want to use, the submission line for \textcolor{red}{symfun\_gen.py} becomes:
%\begin{center}
%overleaf color boxex grey \mybox{\textit{python symfun\_gen.py -e H O -c 14.0 -d 0.2 $>$ asfs.txt}}
%\end{center}

%ASFs are thus generated and stored in the asfs.txt file, which is then appended to the input.nn as
%\begin{center}
%\mybox{\textit{cat asfs.txt $>>$ input.nn}}
%\end{center}

The initial input files needed for the training are therefore:
\begin{itemize}
    \item \textbf{input.data}
    \item \textbf{input.nn}.
\end{itemize}

\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{input.data}
    \item \textbf{input.nn} 
\end{itemize}
\end{mybox3}
%
\subsubsection{Selection of a subset of descriptors}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{n2p2.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item input.data
    \item input.nn
    \item \textcolor{green}{nnp--scaling}
    \item \textcolor{red}{CurSel-integral.py}
    \item \textcolor{red}{curlib.py} / \textcolor{red}{iolib.py} / \textcolor{red}{sflib.py}
\end{itemize}
\end{minipage}
\end{mybox2}
%The first step, once initial files .data and .nn are ready, is to launch the scaling of our neural network. To perform this task you could place in the same directory the .data and .nn files and type in your terminal:

Using the symfunc\_paramgen tool, we generated 192 radial and 768 angular symmetry functions, \textit{i.e.} 480 SF per element. The goal of this section is to selected small subset of the SF which will be used for the training of the NNPs. Following the benchmark and recommendations discussed in Ref. \citenum{Imbalzano2018}, we use CUR decomposition scheme to select 64 unique symmetry functions per element. The details of CUR and more advanced CovCUR techniques can be found in Ref. \citenum{maho-drin09pnas,Imbalzano2018,Cersonsky2021}. 

To evaluate the symmetry functions over the geometries, we use \textit{nnp-scaling} tool in n2p2. However, the number of the symmetry function is huge and the storage of the values over the whole set of structures would be memory demanding., We therefore firstly randomly select subset of structures from the original input.data file using \textit{nnp-select} tool.

\begin{center}
\mybox{\textit{nnp-select random 0.05 123}}
\end{center}

Keyword \textit{random} indicates the random selection of the structures, 0.05 corresponds to the percentage of selection and 123 is the seed for random generator. Modify the percentage so it corresponds to roughly 1000 structures.

Finally, evaluate the symmetry functions by \textit{nnp-scaling}. As this procedure ca be computationally demanding, it is advisable to submit it on cluster in parallel, for example:

\begin{center}
\mybox{\textit{mpirun -n 8 nnp-scaling 500 $>$ logfile}}
\end{center}


\textit{nnp-scaling} generates three output files:
\begin{enumerate}
    \item \textbf{logfile}: contains list of the symmetry functions and their parameters. \textbf{Needed for the CUR selection.}
    \item \textbf{function.data}: the evaluation of all the symmetry functions over tha data in \textit{input.data}. \textbf{Needed for CUR selection.}
    \item \textbf{scaling.data}: statistic of the symmetry functions, \textit{i.e.}, the minimum and maximum values, mean and sigma.
\end{enumerate}

TO BY ADDED!!
%Among all the ASFs generated, it is now essential to choose a reasonably as well as nonredundant set of ASFs which describe as accurately as possible our system. This task is often the most delicate and time--consuming aspects in the construction of a Behler--Parinnello style MLP. We thus decided to automatize this selection using CUR description and implemented in a home script named \textcolor{red}{CurSel-integral.py}. You could just place this script in the same direction as function.data and launch it as:
%\begin{center}
%\mybox{\textit{python CurSel-integral.py function.data logfile -n 64 --landmarks 1000 --prefix cursel $>$ out}}
%\end{center}
%As before, 3 output files are generated:
%\begin{enumerate}
%    \item \textbf{out}: the output file of the procedure.
%    \item \textbf{cursel.def}: the chosen symmetry functions.
%    \item \textbf{cursel.landmarks}: the frame's number selected.
%\end{enumerate}

%The \textbf{cursel.def} will be useful in the STEP 4 and \textbf{cursel.landmarks} in STEP 3. \\
%\begin{mybox1}{Checking of the ASFs !}
%\Warning Generated ASFs  should be checked by plotting them in an external software (gnuplot) in order to check if they well cover the space or not. A good balance between G2 and G3 has also to be respected !
%\end{mybox1}

%\begin{mybox1}{Stability of NNP !}
%\Warning Although a good balance between G2 and G3 is needed, some other parameters are important to consider. One of them is the number of structures within the dataset. In our case, with only 1000 structures we can not wait for a stable NNP as the number of structures should not be sufficient enough. However, we kept this number to ensure a good ratio between understanding and short computational time for methodological understanding within this tutorial. However, this setup should not be used for production. If the user is then, after this tutorial, interested to use it for production, a setup used recently for production is provided at the end of this tutorial ! 
%\end{mybox1}


%\begin{mybox3}{Output Files}
%\begin{itemize}
%    \item logfile
%    \item function.data
%    \item scaling.data
%    \item out
%    \item \textbf{cursel.def}
%    \item \textbf{cursel.landmarks}
%\end{itemize}
%\end{mybox3}
%

\subsection{Step 3: Hybrid DFT force--energy computations}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{Python-fortran.jpeg}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item cursel.landmarks
    \item input.data
    \item \textcolor{red}{selection.f90}
    \item \textcolor{red}{from\_data\_to\_xyz.f90}
    \item \textcolor{red}{from\_xyz\_to\_cp2k.f90}
\end{itemize}
\end{minipage}
\end{mybox2}

From the cursel.landmarks file generated in the previous step, we need to extract structures for the single point reference computations, for example using \textcolor{red}{selection.f90}:

\begin{center}
\mybox{\textit{gfortran selection.f90 -o selection ; ./selection}}
\end{center}
It will generate a new file \textbf{new--input.data} containing the 1000 structures selected by the FPS. In this tutoria, we use cp2k code to generate energy and forces of the water box. However, the reference computations for the NNP in general do not need to be periodic. The geometry input files for cp2k can be generated by: 

\begin{enumerate}
    \item \textit{gfortran from\_data\_to\_xyz.f90 -o from\_data\_to\_xyz ; ./from\_data\_to\_xyz}
    \item \textit{gfortran from\_xyz\_to\_cp2k.f90 -o from\_xyz\_to\_cp2k ; ./from\_xyz\_to\_cp2k}
\end{enumerate}

\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{\$i-cp2k.xyz}
\end{itemize}
\end{mybox3}
%
\subsubsection{CP2K computations}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{CP2K_logo.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item \$i-cp2k.xyz
    \item input.cp2k
    \item \textcolor{red}{prep-file.sh}
\end{itemize}
\end{minipage}
\end{mybox2}

The cp2k computations can be automatized by \textcolor{red}{prepare.sh} script which automatically creates directories for the computations (one per structure). The directories contain geometry files and inputs to cp2k. Here, we use the CP2K 6.1 version to perform reference energies and forces computations at the PBE--D3BJ level of theory. All elements are described with the TZV2P--MOLOPT basis set with cores represented by the dual--space Goedecker--Teter--Hutter pseudopotentials (GTH PBE). The plane--wave cutoff is set to 700 Ry with a relative cutoff of 70 Ry.\\

Launch a cp2k computations following the installation of your machine. For example:
\begin{center}
\mybox{\textit{cp2k.popt -i input.cp2k -o output.cp2k}}
\end{center}
The output of the computation including energy and forces is in the output.cp2k file.
\\
\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{output.cp2k} 
\end{itemize}
\end{mybox3}
%
\subsubsection{Extraction of energy and forces}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{Python-fortran.jpeg}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item output.cp2k
  \item \textcolor{red}{process.sh}
\end{itemize}
\end{minipage}
\end{mybox2}
The database of the energy and forces can be created using the process.sh scrip. The resulting information is saved in database\_forces-pbe0.xyz file.
\\
\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{database\_forces-pbe0.xyz}
\end{itemize}
\end{mybox3}
%
\subsubsection{Final input.data file}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{Python-fortran.jpeg}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item input.data
    \item database\_forces-pbe0.xyz
    %\item \textcolor{red}{update.f90}
\end{itemize}
\end{minipage}
\end{mybox2}
Finally, with the database of energies and forces for the training structures, we create the final input.data file which is used for  the training. This can be done, for instance, by the XXX code.
\begin{center}
\mybox{\textit{gfortran update.f90 ; ./a.out}}
\end{center}
%You now have to replace your old input.data by the %\textbf{new-%input.data} generated as the output of your script for the %next step.
%\\
\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{input.data} 
\end{itemize}
\end{mybox3}
%
\subsection{Step 4: Neural Network Potentials}
%\subsubsection{New scaling.data file}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{n2p2.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
  \item input.data
   \item input.nn
     \item \textcolor{green}{nnp--scaling}
   \item \textcolor{green}{nnp--train}
\end{itemize}
\end{minipage}
\end{mybox2}
The last file we need to prepare for the training of the neural network is the preparation of the new scaling.data file. As in the previous case, we generate it using nnp-scaling in n2p2.
%Before proceeding to the Neural Network training a new scaling.data %has to be generated, with the use of the new files created during the %previous steps:
%\begin{itemize}
 %   \item input.data, coming from the "Updating the input.data file";
  %  \item input.nn, coming from "Preparing files for n2p2"
%\end{itemize}
%You create a new directory encompassing these two files and applied %the \textcolor{green}{nnp--scaling} procedure similar to the point %"Preparing files for n2p2" to generate the right scaling.data file. 
%\\
%begin{mybox3}{Output Files}
%begin{itemize}
%    item \textbf{scaling.data}
%end{itemize}
%end{mybox3}
%
%\subsubsection{Training of the Neural Network}
%\begin{mybox2}{{Input Files, Executables and Scripts}}
%\begin{minipage}[c]{0.5\linewidth}
%\includegraphics[scale=0.1]{n2p2.png}
%\end{minipage}
%\begin{minipage}[c]{0.5\linewidth}
%\begin{itemize}
 %   \item input.data
  %  \item input.nn
   % \item scaling.data
   % \item \textcolor{green}{nnp--train}
%\end{itemize}
%\end{minipage}
%\end{mybox2}
Finally, the training procedures can be launched. Depending on the system, the process can be parallelized and submitted, for example, as:
\begin{center}
\mybox{\textit{mpirun -n 32 nnp--train $>$ output.txt}}
\end{center}
%\begin{mybox4}{Libraries}
%\begin{itemize}
 %   \item module load intel intel-mpi intel-mkl gsl eigen is adviced !
  %  \item specify your n2p2 libraries path as: \\ "export LD\_LIBRARY%\_PATH=\$LD\_LIBRARY\_PATH:$<$path-to-your-n2p2-lib$>$"
%\end{itemize}
%\end{mybox4}
The neural network potentials are trained to achieve as low RMSE on the test set as possible. The data to monitor the training behavior of the NNP are provided in the \textit{learning-curve.out} file. The file contains:
\begin{enumerate}
    \item Column 1: The index of the current epoch
    \item Column 2: RMSE of training energies per atom (physical units)
    \item Column 3: RMSE of test energies per atom (physical units)
    \item Column 4: RMSE of training forces (physical units)
    \item Column 5: RMSE of test forces (physical units)
\end{enumerate}

 The physical units correspond to the units used in the training data. In our case, energy is specified in Hartree (a.u.) and forces in Hartree per Bohr. The training errors discussed in the literature are commonly specified in the meV and meV/$\AA$ or in kcal/mol and kcal/mol/$\AA$. The conversion factors are: 1 a.u = 627.5 kcal/mol = 27.211 eV, 1 Bohr = 0.529177 $\AA$. The learning curves can be plotted in Gnuplot to visualize the training progress. For example, the plotting of the RMSE in error in energy for the training set and test set:

\begin{center}
\mybox{\textit{p 'learning-curve.out' u 1:(\$2*1000*27.11) w l, 'learning-curve.out' u 1:(\$3*1000*27.11) w l }}
\end{center}

\begin{figure}
    \centering
    \includegraphics[scale=0.2]{energy.png}
    \includegraphics[scale=0.2]{forces.png}
    \caption{Energy (direct) and forces (direct) learning curves. The training has been performed on 800 structures and testing on 200 structures.}
    \label{fig:my_label}
\end{figure}

The Behler-Parinello NNP can often achieve RMSE around 1 meV/atom in energy and  less than 100 meV/$\AA$ in forces. The RMSE obtained for the water box show similar trend and can be tested in molecular dynamics.

\newpage
\fancyhead[L]{5. NNP--MD}
\section{Part 2: Link the direct NNP to Molecular Dynamics}
We finally generated our first NNP set. In order to improve it, we need to run it to detect non--physical structures. These ones will be then added to the NNP, which will be retrain and the procedure should be performed as a SCF procedure until no new structures are detected. \\
In our case, we would like to see how to link a "ready" NNP, this is why we will not use our NNP but another one already available in the n2p2 examples directory \textit{interface-LAMMPS/H2O\_RPBE-D3/nnp-data}.
%
\subsection{i--PI code}
Since we have a NNP able to reproduce accurately and fastly the forces and energies of water structures, it should be desirable to couple that to a Molecular Dynamics software. Two options are available to couple n2p2:
\begin{itemize}
    \item The LAMMPS software
    \item The CABANA software
\end{itemize}
Our choice in this tutorial is focused on the first, the LAMMPS software. Once this choice of coupling is done, we need now to efficiently couple both to optimize the simulation time. In this direction, the software i--PI is clearly designed to perform this task as it corresponds to a universal force engine for advanced molecular dynamics. i--PI is an interface for advanced molecular simulations written in Python, designed to be used together with an ab initio evaluation of the interactions between the atoms. The main goal is to decouple the problem of evolving the ionic positions to sample the appropriate thermodynamic ensemble and the problem of computing the inter-atomic forces. It could be thus linked to several software such as LAMMPS, which is our main interest in this tutorial. One other interest to use the i--PI interface is that it suitable for high sampling (Replica--exchange) and free energy computations (linkage with PLUMED available).
%
\subsection{Launching the i--PI interface}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.35]{ipi-logo-alpha.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item input.xml
    \item system.xyz
\end{itemize}
\end{minipage}
\end{mybox2}
i--PI was designed for python 2 (even if a new version will be available as soon as possible for python 3), so please used for instance python2.7 to deal with it. Assuming the i--PI is in your directory, before any calculation you have to type in your directory the following command in order to launch the i--PI interface:
\begin{center}
\mybox{\textit{python2.7 i-pi input.xml $>>$ ipi.out \&}}
\end{center}
It will create an output file named ipi.out, and the i--PI interface will be waiting for any actions you will submit. This will be described in the next subsection. \\
Just a point on the input.xml file. It corresponds to the input file of i--PI and contain all the main informations of the MD, such as temperature, timestep, periodicity, units, thermodynamic ensemble, ...). You could feel free to change it if the conditions are not the same for another system of interest. The most critical block in this file is localized in the \textbf{prng} block with:
\begin{itemize}
    \item \textbf{socket}: keep driver--lammps1 if LAMMPS will be coupled to i--PI.
    \item \textbf{address}: it corresponds to the adress of the node where i--PI will be launched.
    \item \textbf{port}: the port of the node.
    \item \textbf{slots}: do not touch it.
    \item \textbf{timeout}: time for killing calculation without having any communications.
\end{itemize}

\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{ipi.out}
\end{itemize}
\end{mybox3}
%
\subsection{Linking n2p2 with LAMMPS}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.35]{LAMMPS.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item lmp1.in
    \item system.xyz
    \item \textcolor{red}{get\_lammps.sh}
\end{itemize}
\end{minipage}
\end{mybox2}
Once i--PI is running and is waiting for communication with another software(s), we can initialize LAMMPS computation which will serve as an intermediate with n2p2 to compute energy and forces at every timestep of the dynamics and communicate them to i--PI for forces integration. We thus start with classical LAMMPS files, which have to be:
\begin{itemize}
    \item An input file (.in) where keyword for MD is specified,
    \item A coordinate file (.data), which is specific for LAMMPS.
\end{itemize}
Starting from a classical xyz structure, we could translate it in .data file by applying the \textcolor{red}{get\_lammps.sh} script as:
\begin{center}
\mybox{\textit{sh get\_lammps.sh system.xyz}}
\end{center}
Just adapt (if necessary) the script file in function of your system ! In our case, it is designed for our periodic box of water molecules, and an output file named \textbf{initial.data} is generated and correspond to the initial set of coordinates for the LAMMPS calculation. This file is called in lmp1.in as \textit{read\_data ./initial.data}, and mass objects are ordered in function of the initial.data file. For the other lines, a classical example is provided in the lmp1.in and is close to every input file for MD (AMBER, NAMD, Tinker, ...). For more details, a tutorial of LAMMPS will be specified at the end of the tutorial. \\
While i--PI is running and the initial.data and lmp1.in are ready for a LAMMPS calculation, additional keywords have to be added in the lmp1.in to specify the connection between LAMMPS and n2p2:
\begin{itemize}
    \item variable runnerCutoff    equal  7.0000
    \item variable runnerDir       string "/path/to/nnp-data"
    \item pair\_style nnp dir \$\{runnerDir\} showew no showewsum 1 resetew yes maxew 200000 cflength 1.889726 cfenergy 0.036749
    \item pair\_coeff * * \$\{runnerCutoff\}
    \item fix 1 all ipi 192.168.100.1 8766
\end{itemize}
The two first lines are variable declarations. \$\{runnerDir\} is the directory of the nnp data and \$\{runnerCutoff\} is maximum cutoff radius of all symmetry functions. \textbf{The cutoff must be given in LAMMPS length units, even if the neural network potential has been trained using a different unit system}. \\
The pair\_style adds an interaction based on the high-dimensional neural network potential method. It uses an interface to the NNP library, which has to be declared before launching the calculation (see next subsection). \\
Finally, the fix line called the adress and the port where i--PI is running.
\\
\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{initial.data}
\end{itemize}
\end{mybox3}
%
\subsection{Launching LAMMPS with the i--PI interface}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.35]{ipi-logo-alpha.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item lmp1.in
    \item initial.data
    \item \textcolor{blue}{nnp--data} directory
    \item \textcolor{green}{lmp}
\end{itemize}
\end{minipage}
\end{mybox2}
Create the nnp--data directory and place in the following files: input.data, input.nn, scaling.data, weights.XXX.001000.out and change the respective weights001.001000.out and weights008.001000.out by weights.001.data and weights.008.data. \\
Once the lmp1.in and the nnp data directory are ready, just follow these instructions to launch your calculation:
\begin{enumerate}
    \item export LD\_LIBRARY\_PATH=/path/to/n2p2/lib:\$\{LD\_LIBRARY\_PATH\}
    \item lmp -i lmp1.in $>>$ log1.lammps \&
\end{enumerate}
Files \textbf{simulation.force\_0.xyz} and \textbf{simulation.pos\_0.xyz} will be filled, according to the printing frequency we specified in the lmp1.in (here 1 at the "fix" line).
\\
\begin{mybox1}{Interpretation tool !}
\begin{itemize}
    \item Simulation could be observed using either vmd or molden softwares. It is the first step to do in order to observe if the simulation is stable or not.
    \item From this simulation, new structures could then be extracted to improve the quality of the NNP, and training as well as simulations are then performed in a SCF procedure until no new structures are observed.
\end{itemize}
\end{mybox1}
%
\fancyhead[L]{6. Baselined NNP--MD}
\section{Part 3: From direct to baselined NNP}
%
\subsection{What is baselined NNP?}
In the previous sections (parts 1 and 2) we learnt how to generate a NNP and how to use it within the i--PI interface. While the NNP trained directly on DFT data provide predictions at impressive speed, the stability and accuracy of these NNPs is often difficult to achieve. The problems with stability are probably the simplest ones to detect. The system simulated with non-stable NNPs has tendency to explode within the first picosecond of the simulation. The reasons might be:
\begin{enumerate}
    \item \textbf{The quality of the generated data}: instead of generating initial structures with the Tinker MD software, we could use semi--empirical methods to generate structures more closely to \textit{ab initio} dynamics. 
    \item \textbf{The choice of symmetry functions}: the number as well as the spacing chosen to generate the symmetry functions play a crucial role in the stability of the NNP. 
    \item \textbf{Number of data points}: Direct NNP require relatively large number of structures to provide stable dynamics. Low nuber of the structures can be reason for instabilities in sparsely represented regions of PES.
\end{enumerate}
As demonstarted by the examples, stabilization of direct NNPs is not an easy task, even for a small water box. A suitable alternative is to train so called baselined NNP. Instead of reproducing directly the DFT forces and energy, the NNPs are trained to correct the forces and energy computed at a lower level of theory, for example semiempirical method. The baselined NNPs require lower number of structures and descriptors and more robust compared to the direct NNP. In the following part, we show how to adapt our NNP training protocol in order to use it as a "Baselined NNP". 
%
\subsection{Baselined forces and energy}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.15]{xtb.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item input.data
    \item \textcolor{green}{xtb}
\end{itemize}
\end{minipage}
\end{mybox2}
While we already have the accurate reference energies and forces coming from the cp2k computations, we need to choose suitable baseline method. In this tutorial, we use semiempirical GFN0-xTB method. Therefore, energy and forces of the training structures need to be recomputed at this level. \\
To proceed to the computations, please follow these steps:
\begin{enumerate}
    \item Create one folder per structure and extract the corresponded structure from the input.data file and place it in the corresponded folder (\textit{prep.sh})
    \item Convert each frame (n2p2 format) in xyz format (\textit{prep2.sh} linked with \textit{from\_data\_to\_xtb.f90})
    \item Submit interactively all the computations at the GFN0-xTB level of theory (\textit{sub.sh})
    \item Once all the computations are ended, compute the differences between DFT and GFN0-xTB energies and forces for each frame and add the results in a new file called new--input.data (\textit{calc--diff.sh}).
\end{enumerate}

\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{new--input.data}
\end{itemize}
\end{mybox3}
%
\subsection{New training}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.1]{n2p2.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item new--input.data
    \item input.nn
    \item \textcolor{green}{nnp--scaling}
    \item \textcolor{green}{nnp--train}
\end{itemize}
\end{minipage}
\end{mybox2}
First, rename the new--input.data to input.data and place it in a new folder with the input.nn file. The procedure to train a Baselined--NNP is analogous to direct NNP:
\begin{enumerate}
    \item Use the \textcolor{green}{nnp--scaling} executable (see Step 4) to generate the corresponding scaling.data
    \item Use the \textcolor{green}{nnp--train} executable (see Step 4 again) to train Baselined--NNP. 
\end{enumerate}

\begin{mybox3}{Output Files}
\begin{itemize}
    \item \textbf{learning-curve.out}
    \item \textbf{weights.001.001000.out}
    \item \textbf{weights.008.001000.out}
\end{itemize}
\end{mybox3}
%
\subsection{Baselined--NNP with i--PI}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.35]{ipi-logo-alpha.png}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item input.data // input.nn // scaling.data
    \item weights.001.001000.out // weights.008.001000.out
    \item input.xml
    \item initial.data // system.xyz
    \item lmp1.in
    \item \textcolor{green}{xtb\_ase.py} // xtb\_io.py
\end{itemize}
\end{minipage}
\end{mybox2}

Once the training is finished, you can launch your Baselined--NN MD similarly to direct NNP. The main difference is that apart from the NNP itself, you also need to perform computation of the baseline ate every step. The submission is  be done as follows: 
\begin{enumerate}
    \item Create a new directory labeled nnp--data and place there the input.data, input.nn, scaling.data and weightsXXX.data files from the last epoch
    \item Change the name of the weightsXXX.data files in respectively weights.001.data (H) and weights.008.data (O).
    \item Adapt the path of the nnp--data folder in the lmp1.in file
    \item Also adapt the sockets in the respectively files:
    \begin{itemize}
        \item input.xml
        \item lmp1.in
        \item xtb\_ase.py
        \item sub.run
    \end{itemize}
    \item Launch the simulation with the sub.sh script file !
\end{enumerate}

\begin{mybox1}{New xtb--ase python interface}
\begin{itemize}
    \item The both files \textcolor{green}{xtb\_ase.py} and xtb\_io.py provide a new python interface between ase and xtb as the interface for xtb 6.3.3 encounters some issues ...
    \item This interface is a little bit primitive, so be careful about error messages ...
\end{itemize}
\end{mybox1}

\begin{mybox1}{Interpretation tools}
\begin{itemize}
    \item This time, the simulation does not crash as it was the case for the direct NNP. It finally means that direct NNP is definitevely more sensitive to the choice of symmetry functions as well as initial structures encompassing the dataset. 
    \item However, the EW messages present in the log.lammps are too high ... (around 50 EW per step in our cas). \textbf{A rule of thumb is to consider that a structure depicted more than 10/20 EW is badly described by the NNP}. It means that even if the simulation does not crash, the NNP is not optimal and should be optimized following the self consistent procedure described in section 2 of this tutorial.
\end{itemize}
\end{mybox1}
%
\subsection{Fast and accurate: Multiple-time-step approach}
\begin{mybox2}{{Input Files, Executables and Scripts}}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[scale=0.35]{ipi-logo-alpha.png} \\
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\begin{itemize}
    \item input.data // input.nn // scaling.data from baselined
    \item input.data // input.nn // scaling.data from direct
    \item weights.001.001000.out // weights.008.001000.out from baselined
    \item weights.001.001000.out // weights.008.001000.out from direct
    \item input.xml
    \item initial.data // system.xyz
    \item lmp1.in
    \item \textcolor{green}{xtb\_ase.py} // xtb\_io.py
\end{itemize}
\end{minipage}
\end{mybox2}
When direct NNP is not stable to fully run itself but not so bad in single point accuracy, it could be coupled to the baselined NNP to form what we call a mulit--time--step approach. In this approach, forces and energy are computed x times using the direct NNP and then one time with the baselined NNP. It helps to avoid some crashing from the direct NNP and improves the speed of the simulation because the baselined is not computed at every step. Using the i--PI software, we will see how such a coupling is possible. 


To run i--PI within such a mode (i.e multi--time--step), please follow the following instructions:
\begin{enumerate}
    \item Create a folder and place the submission file, system.xyz and input.xml files
    \item In this folder, create two new directories labeled nnp-data-direct and nnp-data-baselined and place in each respective folder the corresponding input.data, input.nn, scaling.data and the weights.XXX.data files
    \item Place the lmp1.in in both the directories and adapt the respective path of the corresponding folder in each lmp1.in
    \item Do not forget to place xtb\_ase.py and xtb\_io.py in the baselined directory
    \item Adapt the sockets in input.xml and lmp1.in for the direct NNP
    \item Adapt the sockets in input.xml, lmp1.in and xtb\_ase.py for the baselined NNP
    \item Launch the simulation with the sub.sh script file !
\end{enumerate}
In our case, we launched a multi--time--step simulation with an inner timestep of 0.5 fs where forces and energy are computed with the direct NNP and an outer timestep of 3 fs where forces and energy are computed using the xTB baseline and corrected with the baselined NNP. All the relative keywords are mentioned within the input.xml and the structure of the file does not differ from the previous ones met during the tutorial.
\begin{mybox1}{Interpretation tools}
\begin{itemize}
    \item The accuracy of such a dynamic is very sensitive to the energy conservation during the dynamics. Therefore, the choice of 0.5/3 was made in a sens to ensure the conservation of the energy. However on other systems careful attention should be taken on this conservation.
    \item As mentioned before EW should still be checked but should be now lower compared to the direct NNP simulation. 
    \item Finally, in order to improve the accuracy from the baselined more than 1 NNP can be considered during the dynamics. Indeed, i--PI enables to take the average made on more than 1 prediction. It has the main advantage to decrease the error made on the NNP prediction but impact the speed of the dynamics as more predictions has to be performed at the same time.
\end{itemize}
\end{mybox1}
%
%
\newpage
\fancyhead[L]{7. Conclusions and supplementary tutorials}
\section{Conclusions}
Thorough this tutorial we described the main idea on how to construct manually an efficient NNP and how to link it to a MD software (LAMMPS in our case). This procedure is straightforwardly transferable to every chemical systems and hope it will be helpful to include reactivity in conventional Molecular Dynamics.  
%
\section{Supplementary tutorials}
\label{sec:tutorials}
A list of supplementary tutorials is given to help users if questions appear (espcially in the input files of several softwares).
\subsection{Tinker(--HP)}
\begin{itemize}
    \item Frédéric CELERSE personnal tutorials (on request): cMD / preparing Tinker xyz files / Umbrella Sampling / Steered Molecular Dynamics / Gaussian accelerated Molecular Dynamics
    \item AMOEBA workshop: \url{https://sites.google.com/site/amoebaworkshop/}
    \item Tinker website: \url{https://dasher.wustl.edu/tinker/}
    \item GitHub (open source code): \url{https://github.com/TinkerTools/tinker}
\end{itemize}
\subsection{n2p2}
\begin{itemize}
    \item GitHub (open source code): \url{https://github.com/CompPhysVienna/n2p2}
    \item Short tutorial: \url{https://compphysvienna.github.io/n2p2/index.html}
\end{itemize}
\subsection{cp2k}
\begin{itemize}
    \item GitHub (open source code): \url{https://github.com/cp2k/cp2k}
    \item cp2k main website: \url{https://www.cp2k.org/}
\end{itemize}
\subsection{i--PI}
\begin{itemize}
    \item GitHub (open source code): \url{https://github.com/i-pi/i-pi}
    \item i--PI website: \url{http://ipi-code.org/}
    \item i--PI manual: \url{http://ipi-code.org/assets/pdf/manual.pdf}
\end{itemize}
\subsection{LAMMPS}
\begin{itemize}
    \item GitHub (open source code): \url{https://github.com/lammps/lammps}
    \item LAMMPS website: \url{https://lammps.sandia.gov/}
    \item LAMMPS tutorials: \url{https://icme.hpc.msstate.edu/mediawiki/index.php/LAMMPS_tutorials.html}
\end{itemize}

\newpage
\fancyhead[L]{9. Optimal protocol for stable NNP}
\section{Optimal protocol for stable NNP}
In this last section, we aim at providing optimal parameters to train a NNP which could be then used for production. Note that the proposed methods and parameters come from different works available in the literature and modifications might be needed. 

\textbf{SYSTEM CASE}: The user want to model a direct NNP of an azobenzene in order to reproduce an \textit{ab--initio} MD. In this case, the user has to separate two important components:
\begin{enumerate}
    \item How he will built his own NNP ?
    \item How he will train for ?
\end{enumerate}
These two features are complementary but have to be treated carefully and separated. Let us see how we suggest to proceed for that !
\subsection{How to build a good starting NNP ?}
The complexity as well as the chemical event that any user would like to explore are two crucial parameters to take into account. Here, there is only 3 different atom types (H,C and N) and the system is made of 24 atoms. In case of the photoswitches, the event could be decoupled into two main parts:
\begin{enumerate}
    \item \textbf{PHOTO}: a photo event comes from by an photo-excitation of the system through any wave length. 
    \item \textbf{SWITCH}: a switch event is related to the conformation switch of a molecule which is only due to thermal fluctuations.
\end{enumerate}
In our case, the photo part needs some external excitations, which is not available in MD--NN based only on the ground state Potential Energy Surface. Concerning the switch part, we have two available conformations: E and Z. One interesting question should be thus: \textbf{Which mechanic way should enable the switch of azobenzene from E to Z ?} \\
To answer to this question, we need thus to prepare the NN according to this specific question. In our case, and according to the literature, azobenzene could depict two switch mechanisms: the inversion and the rotation. The aim is thus to let the choice of the desired path within our MD--NN to observe which way is preferred or not. Therefore, the NN has to know the existence of these two ways and structures taken to build the NN should be part of these pathways. \\
To generate these structures, you can follow the following instructions:
\begin{enumerate}
    \item For each pathway, generate a metadynamics of several ns using a baselined (DFTB+, xTB) using i--Pi coupled to the baselined and plumed,
    \item Extract the most relevant structures either manually or by using a clustering method (PCA, TICA, TSNE, ...)
    \item Only for the most important structures, run a Replica Exchange MD of several ps in order to capture the fluctuation effects from the temperature. Extract as before the most relevant structures.
\end{enumerate}
Once all the structures extracted, we can grab them in order to generate the first dataset which will be used as a starting point for the procedure. Note that a good compromise for the number of structures to start is localized between 5000 (for simple system) and 10 000 (for more complex system). 
%
\subsection{How to efficiently train a good NNP ?}
Once the initial dataset was created, the other important part of the work is to design the architecture for future NNP.

\bibliographystyle{achemso}
\bibliography{bib}

\end{document}